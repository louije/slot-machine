# slot-machine spec v2

## Philosophy

Three slots, not two. Append-only. Copy-on-write.

v1 swapped two named slots (a/b) back and forth. This conflated the rollback
target with the agent workspace — if the agent is mid-edit on the idle slot and
you need to rollback, you'd restart dirty state.

v2 separates the three roles cleanly:

| Role        | What it is                                                    |
|-------------|---------------------------------------------------------------|
| **live**    | Serving traffic. Never touched.                               |
| **prev**    | Last known-good. Untouched. Rollback restarts it.             |
| **staging** | Workspace. Agent edits, tests, commits here. Next deploy src. |

Deploys append to a journal. Old slots get garbage-collected. Slot directories
are named by commit hash (`slot-<hash>`), staging is always `slot-staging`.

## Slot lifecycle

```
  deploy A:   [staging] ──promote──▶ [live:A]        + new staging
  deploy B:   [staging] ──promote──▶ [live:B, prev:A] + new staging
  deploy C:   [staging] ──promote──▶ [live:C, prev:B] + new staging  (A is GC'd)
  rollback:   prev B restarted ──▶   [live:B, prev:C] + new staging  (swap)
```

## Directory layout

```
.slot-machine/
  journal.ndjson          # append-only deploy log
  slot-staging/           # the workspace (git worktree)
  slot-abc1234/           # deployed slot (git worktree, checked out at abc1234)
  slot-def5678/           # another deployed slot
  slot-abc1234.log        # stdout/stderr for that slot's process
```

Slot directories are named `slot-<short-hash>` where `<short-hash>` is the
first 8 characters of the commit. The full commit is recorded in the journal.

## Journal

`journal.ndjson` — one JSON object per line, append-only. The journal is the
source of truth for which slots exist and what role they hold. It survives
daemon restarts.

```jsonl
{"time":"2025-06-01T12:00:00Z","action":"deploy","commit":"abc12345...","slot_dir":"slot-abc1234","prev_commit":"","status":"ok"}
{"time":"2025-06-01T13:00:00Z","action":"deploy","commit":"def56789...","slot_dir":"slot-def5678","prev_commit":"abc12345...","status":"ok"}
{"time":"2025-06-01T14:00:00Z","action":"rollback","commit":"abc12345...","slot_dir":"slot-abc1234","prev_commit":"def56789...","status":"ok"}
```

On startup, the daemon reads the journal to reconstruct state: the last
successful entry determines live/prev. If the journal is missing or empty,
there is no live slot (fresh start).

## Deploy sequence

```
  POST /deploy {commit}

  1. Lock (reject concurrent deploys)
  2. git checkout <commit> in slot-staging
  3. Run setup_command in slot-staging
  4. Start app process in slot-staging (dynamic port)
  5. Health check on dynamic port
  6. If healthy:
       a. Rename slot-staging → slot-<hash>    (or mv via git worktree move)
       b. Switch proxy: route traffic to new slot's port
       c. Drain old live process (SIGTERM → timeout → SIGKILL)
       d. Stop old prev process if running, delete its worktree (GC)
       e. Create new slot-staging (clone of slot-<hash>)
       f. Append to journal
       g. Respond {success: true}
  7. If unhealthy:
       a. Kill the staging process
       b. Respond {success: false}
       c. Staging stays as-is (agent can inspect/fix)
```

**Key difference from v1**: the old live process keeps serving traffic through
steps 2-5. There is no downtime window. The proxy switches atomically in step
6b, then the old process drains.

### Dynamic ports

Each slot's app runs on a port assigned by the daemon (not the configured
`port`). The daemon reverse-proxies the configured `port` to whichever slot is
live. The `internal_port` is also dynamically assigned for health checks.

The app receives its port via the `PORT` and `INTERNAL_PORT` env vars, same as
v1. The difference is that these values are chosen by the daemon per slot, not
read from the config.

Config `port` = the stable external port the proxy listens on.
Config `internal_port` = removed (dynamic).

### Staging creation

After promoting staging to live, a new staging directory is needed. Two
strategies, in order of preference:

1. **CoW clone** (`cp -c` on APFS/btrfs) of the just-promoted slot. Instant,
   zero disk cost. The new staging has node_modules, build artifacts, etc.
   already in place. Then `git worktree add` to register it as a worktree at
   the same commit.

2. **Fresh worktree** (`git worktree add slot-staging <commit>`). Clean but
   cold — setup_command must run before the next deploy.

The implementation should try (1) and fall back to (2).

## Rollback sequence

```
  POST /rollback

  1. Lock
  2. Verify prev slot exists
  3. Start app process in prev slot (dynamic port)
  4. Health check
  5. If healthy:
       a. Switch proxy to prev slot's port
       b. Drain old live
       c. Old live becomes new prev (no GC — it's still recent)
       d. Create new staging (clone of newly promoted prev)
       e. Append to journal
       f. Respond {success: true}
  6. If unhealthy:
       a. Kill the process
       b. Respond {success: false}
       c. Nothing changes
```

Rollback is just a deploy of the prev commit, but without checkout/setup
(the worktree is already there, ready to go).

## Garbage collection

Only three slot directories exist at any time: staging, live, prev.

When a new deploy promotes staging to live:
- The old prev's process is stopped (if running) and its worktree is deleted
  (`git worktree remove`)
- The old live becomes prev (process drained, worktree kept)
- A new staging is cloned from the promoted slot

This keeps disk usage bounded. A future extension could keep N previous slots
for deeper rollback history.

## HTTP API

### POST /deploy

Request: `{"commit": "<hash>"}`

Response:
```json
{
  "success": true,
  "slot": "slot-abc1234",
  "commit": "abc12345...",
  "previous_commit": "def56789..."
}
```

### POST /rollback

Response:
```json
{
  "success": true,
  "slot": "slot-def5678",
  "commit": "def56789..."
}
```

### GET /status

```json
{
  "live_slot": "slot-abc1234",
  "live_commit": "abc12345...",
  "prev_slot": "slot-def5678",
  "prev_commit": "def56789...",
  "staging_dir": "slot-staging",
  "healthy": true,
  "last_deploy_time": "2025-06-01T12:00:00Z"
}
```

## Config changes from v1

```json
{
  "start_command": "bun server/index.ts",
  "setup_command": "bun install --frozen-lockfile",
  "port": 3000,
  "health_endpoint": "/healthz",
  "health_timeout_ms": 10000,
  "drain_timeout_ms": 5000,
  "env_file": ".env",
  "api_port": 9100
}
```

Removed: `internal_port` (now dynamic, assigned per slot by the daemon).

## Test scenarios

All v1 tests still apply (with adjusted expectations for slot naming). New
tests for v2 behavior:

| #  | Test | What it validates |
|----|------|-------------------|
| 18 | `TestZeroDowntime` | Old app serves traffic during deploy until new app is healthy |
| 19 | `TestStagingSlotExists` | After deploy, status shows a staging directory |
| 20 | `TestStagingPreservesArtifacts` | New staging contains files from promoted slot (e.g., marker file written by setup_command) |
| 21 | `TestJournalPersistsDaemonRestart` | Deploy, restart daemon, status still shows live slot |
| 22 | `TestGarbageCollection` | After three deploys (A→B→C), slot-A's worktree is removed |
| 23 | `TestRollbackThenDeploy` | Rollback, then deploy new commit — full cycle works |
| 24 | `TestDynamicPorts` | Two slots can run simultaneously on different ports during handoff |

### TestZeroDowntime

1. Deploy commit A, verify serving
2. Start deploying commit B (slow boot — 3s delay)
3. While B is booting, verify A still responds on the public port
4. Wait for deploy to complete
5. Verify B is now serving, A is drained

This is the signature v2 test — v1 would fail it because v1 drains before
starting the new process.

## Changes from v1

| Aspect | v1 | v2 |
|--------|----|----|
| Slots | 2 (a/b), swap | 3 roles (prev/live/staging) |
| Naming | slot-a, slot-b | slot-\<hash\>, slot-staging |
| Deploy order | drain old → start new | start new → health → drain old |
| Downtime | brief (drain → start gap) | zero (proxy switch) |
| Port model | single port, one app at a time | dynamic ports, proxy on stable port |
| State | in-memory | journal (survives restarts) |
| Staging after deploy | old live reused as-is | CoW clone of promoted slot |
| Rollback target | might be dirty (agent editing) | always clean (never touched) |
| GC | none (2 slots reused) | old prev deleted after new deploy |
