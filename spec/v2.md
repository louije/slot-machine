# slot-machine spec v2

## Philosophy

Three slots, not two. Append-only. Copy-on-write.

v1 swapped two named slots (a/b) back and forth. This conflated the rollback
target with the agent workspace — if the agent is mid-edit on the idle slot and
you need to rollback, you'd restart dirty state.

v2 separates the three roles cleanly:

| Role        | What it is                                                    |
|-------------|---------------------------------------------------------------|
| **live**    | Serving traffic. Never touched.                               |
| **prev**    | Last known-good. Untouched. Rollback restarts it.             |
| **staging** | Workspace. Agent edits, tests, commits here. Next deploy src. |

Old slots get garbage-collected. Slot directories are named by commit hash
(`slot-<hash>`), staging is always `slot-staging`.

## Slot lifecycle

```
  deploy A:   [staging] ──promote──▶ [live:A]        + new staging
  deploy B:   [staging] ──promote──▶ [live:B, prev:A] + new staging
  deploy C:   [staging] ──promote──▶ [live:C, prev:B] + new staging  (A is GC'd)
  rollback:   prev B restarted ──▶   [live:B, prev:C] + new staging  (swap)
```

## State

Operational state is stored as symlinks in the data directory:

```
.slot-machine/
  live → slot-abc1234        # symlink to the live slot directory
  prev → slot-def5678        # symlink to the rollback target
  slot-staging/              # the workspace (git worktree)
  slot-abc1234/              # deployed slot (git worktree, checked out at abc1234)
  slot-def5678/              # another deployed slot
  slot-abc1234.log           # stdout/stderr for that slot's process
```

Symlinks are atomic on POSIX — updating `live` is a single `rename(2)` call.
On daemon startup, the daemon reads the symlinks to reconstruct state. If
`live` points to a valid slot directory, restart its process. If `live` is
missing or dangling, treat as fresh start.

Slot directories are named `slot-<short-hash>` where `<short-hash>` is the
first 8 characters of the commit.

### Journal (optional)

`journal.ndjson` — one JSON object per line, append-only. The journal is an
audit log, not a correctness dependency. The daemon appends to it on every
deploy/rollback but does not need it to operate. If missing or corrupt,
nothing breaks — you just lose history.

```jsonl
{"time":"2025-06-01T12:00:00Z","action":"deploy","commit":"abc12345...","slot_dir":"slot-abc1234","prev_commit":""}
{"time":"2025-06-01T13:00:00Z","action":"deploy","commit":"def56789...","slot_dir":"slot-def5678","prev_commit":"abc12345..."}
{"time":"2025-06-01T14:00:00Z","action":"rollback","commit":"abc12345...","slot_dir":"slot-abc1234","prev_commit":"def56789..."}
```

The journal is useful for UI, debugging, and audit trails. It is not read on
startup — symlinks are the source of truth.

## Deploy sequence

```
  POST /deploy {commit}

  1. Lock (reject concurrent deploys)
  2. git checkout <commit> in slot-staging
  3. Run setup_command in slot-staging
  4. Start app process in slot-staging (dynamic port)
  5. Health check on dynamic port
  6. If healthy:
       a. Rename slot-staging → slot-<hash>    (or mv via git worktree move)
       b. Update live symlink → slot-<hash>
       c. Switch proxy: route traffic to new slot's port
       d. Drain old live process (SIGTERM → timeout → SIGKILL)
       e. Update prev symlink → old live's slot dir
       f. Stop old prev process if running, delete its worktree (GC)
       g. Create new slot-staging (clone of slot-<hash>)
       h. Append to journal (best-effort)
       i. Respond {success: true}
  7. If unhealthy:
       a. Kill the staging process
       b. Respond {success: false}
       c. Staging stays as-is (agent can inspect/fix)
```

**Key difference from v1**: the old live process keeps serving traffic through
steps 2-5. There is no downtime window. The proxy switches atomically in step
6c, then the old process drains.

### Startup

On startup, the daemon reads symlinks to reconstruct state:

1. Read `live` symlink → determine live slot directory
2. If live slot dir exists → start its process, health check
3. If live slot dir missing → clear symlink, log warning, treat as fresh
4. Read `prev` symlink → if target exists, note it; if dangling, remove
5. If no `live` symlink → fresh start (no live slot)

### Dynamic ports

Each slot's app runs on a port assigned by the daemon (not the configured
`port`). The daemon reverse-proxies the configured `port` to whichever slot is
live. The health check port is also dynamically assigned.

The app receives its port via the `PORT` and `INTERNAL_PORT` env vars, same as
v1. The difference is that these values are chosen by the daemon per slot, not
read from the config.

Config `port` = the stable external port the proxy listens on.
Config `internal_port` = removed (dynamic).

### Staging creation

After promoting staging to live, a new staging directory is needed. Two
strategies, in order of preference:

1. **CoW clone** (`cp -c` on APFS/btrfs) of the just-promoted slot. Instant,
   zero disk cost. The new staging has node_modules, build artifacts, etc.
   already in place. Then `git worktree add` to register it as a worktree at
   the same commit.

2. **Fresh worktree** (`git worktree add slot-staging <commit>`). Clean but
   cold — setup_command must run before the next deploy.

The implementation should try (1) and fall back to (2).

## Rollback sequence

```
  POST /rollback

  1. Lock
  2. Verify prev symlink exists and target dir is valid
  3. Start app process in prev slot (dynamic port)
  4. Health check
  5. If healthy:
       a. Switch proxy to prev slot's port
       b. Drain old live
       c. Update prev symlink → old live's slot dir
       d. Update live symlink → prev slot dir
       e. Create new staging (clone of newly promoted slot)
       f. Append to journal (best-effort)
       g. Respond {success: true}
  6. If unhealthy:
       a. Kill the process
       b. Respond {success: false}
       c. Nothing changes
```

Rollback is just a deploy of the prev commit, but without checkout/setup
(the worktree is already there, ready to go).

## Garbage collection

Only three slot directories exist at any time: staging, live, prev.

When a new deploy promotes staging to live:
- The old prev's process is stopped (if running) and its worktree is deleted
  (`git worktree remove`)
- The old live becomes prev (process drained, worktree kept)
- A new staging is cloned from the promoted slot

This keeps disk usage bounded. A future extension could keep N previous slots
for deeper rollback history.

## HTTP API

### POST /deploy

Request: `{"commit": "<hash>"}`

Response:
```json
{
  "success": true,
  "slot": "slot-abc1234",
  "commit": "abc12345...",
  "previous_commit": "def56789..."
}
```

### POST /rollback

Response:
```json
{
  "success": true,
  "slot": "slot-def5678",
  "commit": "def56789..."
}
```

### GET /status

```json
{
  "live_slot": "slot-abc1234",
  "live_commit": "abc12345...",
  "prev_slot": "slot-def5678",
  "prev_commit": "def56789...",
  "staging_dir": "slot-staging",
  "healthy": true,
  "last_deploy_time": "2025-06-01T12:00:00Z"
}
```

## Config changes from v1

```json
{
  "start_command": "bun server/index.ts",
  "setup_command": "bun install --frozen-lockfile",
  "port": 3000,
  "health_endpoint": "/healthz",
  "health_timeout_ms": 10000,
  "drain_timeout_ms": 5000,
  "env_file": ".env",
  "api_port": 9100
}
```

Removed: `internal_port` (now dynamic, assigned per slot by the daemon).

## Test scenarios

All v1 tests still apply (with adjusted expectations for slot naming). New
tests for v2 behavior:

| #  | Test | What it validates |
|----|------|-------------------|
| 18 | `TestZeroDowntime` | Old app serves traffic during deploy until new app is healthy |
| 19 | `TestStagingSlotExists` | After deploy, status shows a staging directory |
| 20 | `TestStagingPreservesArtifacts` | New staging contains files from promoted slot (e.g., marker file written by setup_command) |
| 21 | `TestSymlinksOnDisk` | After deploy, `live` and `prev` symlinks exist and point to correct dirs |
| 22 | `TestDaemonRestart` | Deploy, restart daemon, status still shows live slot |
| 23 | `TestGarbageCollection` | After three deploys (A→B→C), slot-A's worktree is removed |
| 24 | `TestRollbackThenDeploy` | Rollback, then deploy new commit — full cycle works |
| 25 | `TestDynamicPorts` | Two slots can run simultaneously on different ports during handoff |

### TestZeroDowntime

1. Deploy commit A, verify serving
2. Start deploying commit B (slow boot — 3s delay)
3. While B is booting, verify A still responds on the public port
4. Wait for deploy to complete
5. Verify B is now serving, A is drained

This is the signature v2 test — v1 would fail it because v1 drains before
starting the new process.

## Changes from v1

| Aspect | v1 | v2 |
|--------|----|----|
| Slots | 2 (a/b), swap | 3 roles (prev/live/staging) |
| Naming | slot-a, slot-b | slot-\<hash\>, slot-staging |
| Deploy order | drain old → start new | start new → health → drain old |
| Downtime | brief (drain → start gap) | zero (proxy switch) |
| Port model | single port, one app at a time | dynamic ports, proxy on stable port |
| State | in-memory | symlinks (survive restarts) |
| Audit | none | journal.ndjson (optional) |
| Staging after deploy | old live reused as-is | CoW clone of promoted slot |
| Rollback target | might be dirty (agent editing) | always clean (never touched) |
| GC | none (2 slots reused) | old prev deleted after new deploy |
